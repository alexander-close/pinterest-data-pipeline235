{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b9825f-a1f4-4eb1-af32-62baccd0acb7",
     "showTitle": true,
     "title": "Credentials"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import * # pyspark functions\n",
    "import urllib # URL processing\n",
    "import base64\n",
    "import json\n",
    "\n",
    "# set auth credentials from 'Delta table'\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec1cdc7-4156-4a06-bdf0-4d2fad7c968e",
     "showTitle": true,
     "title": "ONLY RUN WHILST STREAMING"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.databricks.delta.formatCheck.enabled</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "spark.databricks.delta.formatCheck.enabled",
         "false"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b1ad360-a599-4c4d-ab37-3d95e30ff973",
     "showTitle": true,
     "title": "Reading in data from Kinesis stream"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pin = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName','streaming-126ca3664fbb-pin') \\\n",
    "    .option('initialPosition','earliest') \\\n",
    "    .option('region','us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "# to decode `data` (from base64) into readable form (JSON string?)\n",
    "df_pin = df_pin.selectExpr(\"CAST(data as STRING)\") # or just use .cast('string') in parsing below\n",
    "\n",
    "\n",
    "df_geo = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName','streaming-126ca3664fbb-geo') \\\n",
    "    .option('initialPosition','earliest') \\\n",
    "    .option('region','us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "df_geo = df_geo.selectExpr(\"CAST(data as STRING)\") # as above\n",
    "\n",
    "df_user = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName','streaming-126ca3664fbb-user') \\\n",
    "    .option('initialPosition','earliest') \\\n",
    "    .option('region','us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "df_user = df_user.selectExpr(\"CAST(data as STRING)\") # as above\n",
    "\n",
    "\n",
    "# manually determined schemas\n",
    "\n",
    "pin_schema = StructType([\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"downloaded\", IntegerType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "geo_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "user_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# converting `data` JSON string into spark dataframe\n",
    "\n",
    "parsed_pin = df_pin.withColumn(\"parsed_data\", from_json(col(\"data\"), pin_schema))\n",
    "df_pin = parsed_pin.select(\"parsed_data.*\")\n",
    "\n",
    "parsed_geo = df_geo.withColumn(\"parsed_data\", from_json(col(\"data\"), geo_schema))\n",
    "df_geo = parsed_geo.select(\"parsed_data.*\")\n",
    "\n",
    "parsed_user = df_user.withColumn(\"parsed_data\", from_json(col(\"data\"), user_schema))\n",
    "df_user = parsed_user.select(\"parsed_data.*\")\n",
    "\n",
    "\n",
    "# display(df_pin)\n",
    "# display(df_geo)\n",
    "# display(df_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24467d6-b7ea-4d8b-be8f-894738eda0a7",
     "showTitle": true,
     "title": "Clean & transform"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[94]: DataFrame[ind: int, user_name: string, age: int, date_joined: timestamp]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[94]: DataFrame[ind: int, user_name: string, age: int, date_joined: timestamp]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# function to convert string numbers to ints, including those with k/M units\n",
    "def convert_to_int(value):\n",
    "    if type(value) == int:\n",
    "        return value\n",
    "    # elif type(value) == float:\n",
    "    #     return value\n",
    "    else:\n",
    "        try:\n",
    "            value = value.strip()\n",
    "            if value.endswith('k'):\n",
    "                return int(float(value[:-1]) * 1000)\n",
    "            elif value.endswith('M'):\n",
    "                return int(float(value[:-1]) * 1000000)\n",
    "            else:\n",
    "                return int(value)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "# custom UDF from the function\n",
    "convert_to_int_udf = udf(lambda x: convert_to_int(x), IntegerType())\n",
    "\n",
    "############# PIN DATA ##############\n",
    "\n",
    "# first cleaning empty strings to None\n",
    "df_pin = df_pin.select(\n",
    "    [\n",
    "        when(trim(col(c)) == \"\", lit(None)).otherwise(col(c)).alias(c) for c in df_pin.columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# applying the UDF to the 1 (string) numerical columns\n",
    "df_pin = df_pin.withColumn(\n",
    "    \"follower_count\",\n",
    "    when(col(\"follower_count\").isNotNull(), convert_to_int_udf(col(\"follower_count\")))\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "# rename and reorder columns\n",
    "df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# reordering  columns\n",
    "df_pin = df_pin.select(\"ind\", \"unique_id\", \"title\",\n",
    "               \"description\", \"follower_count\",\n",
    "               \"poster_name\", \"tag_list\",\n",
    "               \"is_image_or_video\", \"image_src\",\n",
    "               \"save_location\", \"category\",\n",
    "               \"downloaded\") # this last one is actually missing from the instructions\n",
    "\n",
    "# calling datafram\n",
    "df_pin\n",
    "\n",
    "############# GEO DATA ##############\n",
    "\n",
    "# producing geospatial array and dropping original 2 columns\n",
    "df_geo = df_geo.withColumn(\"coordinates\",\n",
    "                           array(col(\"latitude\"), col(\"longitude\"))\n",
    "                           ).drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# convert the datetime string to proper datetime format\n",
    "df_geo = df_geo.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "\n",
    "# reordering\n",
    "df_geo = df_geo.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "\n",
    "# calling datafram\n",
    "df_geo\n",
    "\n",
    "############# USER DATA ##############\n",
    "\n",
    "# combine first and last names then drop original columns\n",
    "df_user = df_user.withColumn('user_name',\n",
    "                             concat_ws(' ',\n",
    "                                       col('first_name'),\n",
    "                                       col('last_name'))).drop('first_name', 'last_name')\n",
    "\n",
    "# convert date_joined to timestamp format\n",
    "df_user = df_user.withColumn(\"date_joined\", to_timestamp(\"date_joined\", \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "\n",
    "# reordering\n",
    "df_user = df_user.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "\n",
    "# calling datafram\n",
    "df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d204813-8e1c-4953-a171-b6a65e53d00d",
     "showTitle": true,
     "title": "Write data to Databricks"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[96]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f79a9f93e80&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[96]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f79a9f93e80&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df_pin.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"126ca3664fbb_pin_table\")\n",
    "\n",
    "df_geo.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"126ca3664fbb_geo_table\")\n",
    "\n",
    "df_user.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"126ca3664fbb_user_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15ea3c9f-c36b-40d0-bc2b-b35698c5a3e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4284316244845221,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Kinesis streaming",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
