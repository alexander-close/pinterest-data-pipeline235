{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, when, lit, trim, array, concat_ws, to_timestamp, from_json\n",
    "from pyspark.sql.types import IntegerType, StructType\n",
    "from pyspark.sql import DataFrame\n",
    "import yaml\n",
    "\n",
    "with open('./mount_config','r') as file:\n",
    "  configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"/mnt/126ca3664fbb-mount/topics/{126ca3664fbb}.{}/partition=0/*.json\"\n",
    "\n",
    "def read_from_s3(\n",
    "    table_name:str,\n",
    "    infer_schema:str=\"true\" # Ask Spark to infer the schema\n",
    ") -> DataFrame:\n",
    "  '''\n",
    "  Reads in json file from S3 bucket,\n",
    "  returns spark DataFrame.\n",
    "  '''\n",
    "  df = spark.read.format(\"json\") \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .load(FILE_PATH.format(table_name))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def none_empty_str(df: DataFrame) -> DataFrame:\n",
    "  '''\n",
    "  Cleans empty string values '' to `None`.\n",
    "  '''\n",
    "  df = df.select(\n",
    "    [\n",
    "        when(trim(col(c)) == \"\", lit(None)).otherwise(col(c)).alias(c) for c in df.columns\n",
    "    ]\n",
    "  )\n",
    "  return df\n",
    "\n",
    "def convert_followers_int(df: DataFrame) -> DataFrame:\n",
    "\n",
    "    def convert_str_int(value: str | int) -> int | None:\n",
    "        '''\n",
    "        Subfunction convert string numbers to ints,\n",
    "         including those with 'k' or 'M' units.\n",
    "        '''\n",
    "        if type(value) == int:\n",
    "            return value\n",
    "        else:\n",
    "            try:\n",
    "                value = value.strip()\n",
    "                if value.endswith('k'):\n",
    "                    return int(float(value[:-1]) * 1000)\n",
    "                elif value.endswith('M'):\n",
    "                    return int(float(value[:-1]) * 1000000)\n",
    "                else:\n",
    "                    return int(value)\n",
    "            except ValueError:\n",
    "                return None\n",
    "    \n",
    "    convert_str_int_udf = udf(lambda x: convert_str_int(x), IntegerType())\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"follower_count\",\n",
    "        when(col(\"follower_count\").isNotNull(), convert_str_int_udf(col(\"follower_count\")))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def rename_index(df: DataFrame) -> DataFrame:\n",
    "    '''\n",
    "    Renames the `index` column to `ind`.\n",
    "    '''\n",
    "    df = df.withColumnRenamed(\"index\", \"ind\")\n",
    "    return df\n",
    "\n",
    "def reorder_pin_cols(df: DataFrame) -> DataFrame:\n",
    "    '''\n",
    "    Reorders the columns of the `pin` table.\n",
    "    '''\n",
    "    df = df.select(\n",
    "        \"ind\", \"unique_id\", \"title\",\n",
    "        \"description\", \"follower_count\",\n",
    "        \"poster_name\", \"tag_list\",\n",
    "        \"is_image_or_video\", \"image_src\",\n",
    "        \"save_location\", \"category\",\n",
    "        \"downloaded\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def clean_pin_table(df: DataFrame) -> DataFrame:\n",
    "    '''\n",
    "    Overall cleaning function for the `pin` table\n",
    "    which incorporates the previous functions of\n",
    "    this cell.\n",
    "    '''\n",
    "    df = none_empty_str(df)\n",
    "    df = convert_followers_int(df)\n",
    "    df = rename_index(df)\n",
    "    df = reorder_pin_cols(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_lat_long(df: DataFrame) -> DataFrame:\n",
    "  '''\n",
    "  Merges latitude and longitude data into an array,\n",
    "  and drops the original two columns.\n",
    "  '''\n",
    "  df = df.withColumn(\n",
    "    \"coordinates\",\n",
    "    array(col(\"latitude\"), col(\"longitude\"))\n",
    "  ).drop(\"latitude\", \"longitude\")\n",
    "  return df\n",
    "\n",
    "def convert_str_datetime(df: DataFrame, col: str) -> DataFrame:\n",
    "  '''\n",
    "  Convert the JSON string datetime to proper datetime format.\n",
    "  '''\n",
    "  df = df.withColumn(col, to_timestamp(col, \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "  return df\n",
    "\n",
    "def reorder_geo_cols(df: DataFrame) -> DataFrame:\n",
    "  '''\n",
    "  Reorders the columns in te `geo` table.'''\n",
    "  df = df.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "  return df\n",
    "\n",
    "def clean_geo_table(df: DataFrame) -> DataFrame:\n",
    "  '''\n",
    "    Overall cleaning function for the `geo` table\n",
    "    which incorporates the previous functions of\n",
    "    this cell.\n",
    "    '''\n",
    "  df = make_lat_long(df)\n",
    "  df = convert_str_datetime(df, \"timestamp\")\n",
    "  df = reorder_geo_cols(df)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_name(df: DataFrame) -> DataFrame:\n",
    "  '''\n",
    "  Combines first and last names into single username.\n",
    "  '''\n",
    "  df = df.withColumn(\n",
    "    'user_name',\n",
    "    concat_ws(\n",
    "      ' ',\n",
    "      col('first_name'),\n",
    "      col('last_name')\n",
    "    )\n",
    "  ).drop('first_name', 'last_name')\n",
    "  return df\n",
    "\n",
    "def reorder_user_cols(df: DataFrame) -> DataFrame:\n",
    "  '''\n",
    "  Reorders `user` columns.\n",
    "  '''\n",
    "  df = df.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "  return df\n",
    "\n",
    "def clean_user_table(df: DataFrame) -> DataFrame:\n",
    "  '''\n",
    "    Overall cleaning function for the `user` table\n",
    "    which incorporates the previous functions of\n",
    "    this cell.\n",
    "    '''\n",
    "  df = make_user_name(df)\n",
    "  df = convert_str_datetime(df, \"date_joined\")\n",
    "  df = reorder_user_cols(df)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kinesis(\n",
    "    table_name:str,\n",
    "    access_key:str ,\n",
    "    secret_key:str,\n",
    "    user_id:str=configs['USER_ID']\n",
    ") -> DataFrame:\n",
    "  '''\n",
    "  Reads in an AWS Kinesis stream as a DataFrame.\n",
    "  '''\n",
    "  df = spark.readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'streaming-{}-{}'.format(user_id, table_name)) \\\n",
    "    .option('initialPosition', 'earliest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('awsAccessKey', access_key) \\\n",
    "    .option('awsSecretKey', secret_key) \\\n",
    "    .load()\n",
    "  # Decodes `data` (from base64) into readable form (JSON string)\n",
    "  df = df.selectExpr(\"CAST(data as STRING)\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_df(df:DataFrame, schema:StructType) -> DataFrame:\n",
    "  '''\n",
    "  Takes an unparsed DataFrame and parses it with the given schema.\n",
    "  '''\n",
    "  df = df.withColumn(\"parsed_data\", from_json(col(\"data\"), schema))\n",
    "  df = df.select(\"parsed_data.*\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_kinesis(df:DataFrame, table_name:str) -> DataFrame:\n",
    "  '''\n",
    "  Stores processed DataFrame by appending to Delta table.\n",
    "  '''\n",
    "  return (\n",
    "    df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\")\n",
    "    .table(\"126ca3664fbb_{}_table\".format(table_name))\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
